{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "gnb = GaussianNB()\n",
    "dec_tree = tree.DecisionTreeClassifier()\n",
    "rfc = RandomForestClassifier(max_depth=5, random_state=42)\n",
    "abc = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "mlpc = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(100,), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words ]\n",
    "    \n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text_by_pos(text, pos_to_keep):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    filtered_tokens = [token for token, pos in tagged_tokens if pos in pos_to_keep]\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize(data):\n",
    "    if type(data[0]) == list:\n",
    "        data = [' '.join(d) for d in data]\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    \n",
    "    return tfidf_vectorizer.fit_transform(data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_vectorize(data):\n",
    "    model = Word2Vec(sentences=data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    def document_vector(model, tokenized_document):\n",
    "        vectors = [model.wv[word] for word in tokenized_document if word in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.array([document_vector(model, doc) for doc in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_vectorize(data):\n",
    "    num_topics = 10\n",
    "    text_vectors = []\n",
    "\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "\n",
    "    model = models.LdaModel(bow_corpus, id2word=dictionary, num_topics=num_topics, passes=15)\n",
    "\n",
    "    for doc_bow in bow_corpus:\n",
    "        document_topics = model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    "        document_topic_vector = [topic_prob for _, topic_prob in document_topics]\n",
    "        text_vectors.append(document_topic_vector)\n",
    "\n",
    "    return np.array(text_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model, train_data, test_data, train_target, test_target):\n",
    "    model.fit(train_data, train_target)\n",
    "\n",
    "    predictions = model.predict(test_data)\n",
    "\n",
    "    f1 = f1_score(test_target, predictions, average='weighted')\n",
    "\n",
    "    res = f'F1 Score: {f1:.4f}'\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset, tokinizers, classifiers):\n",
    "    df = pd.DataFrame(\"\", index=[tokinizer for tokinizer in tokinizers], columns=[classifier for classifier in classifiers])\n",
    "\n",
    "    preprocessed_data = [preprocess_text(text) for text in dataset.data]\n",
    "    \n",
    "    n = len(preprocessed_data)\n",
    "    print(n)\n",
    "\n",
    "    noun_adj_data = [filter_text_by_pos(text, pos_to_keep=['NN', 'NNS', 'JJ', 'JJR', 'JJS']) for text in preprocessed_data]\n",
    "\n",
    "    preprocessed_data = [text.split(' ') for text in preprocessed_data]\n",
    "    \n",
    "    list_of_data = {\"ALL\": preprocessed_data, \"NOUNS and ADJ\": noun_adj_data}\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        print(classifier)\n",
    "\n",
    "        for name, data in list_of_data.items():\n",
    "            print(\"start calculate \" + name)\n",
    "\n",
    "            for tokinizer in tokinizers:\n",
    "                print(tokinizer)\n",
    "                vectors = tokinizers[tokinizer](data)\n",
    "\n",
    "                train_data, test_data, train_target, test_target = train_test_split(\n",
    "                        vectors, dataset.target, test_size=0.2, random_state=42)\n",
    "\n",
    "                print(train_data.shape, test_data.shape)\n",
    "\n",
    "            \n",
    "                df.loc[tokinizer, classifier] = df.loc[tokinizer, classifier] + name + \": \" + classify(classifiers[classifier], train_data, test_data, train_target, test_target) + \"\\n\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokinizers = {\n",
    "    \"tfidf\": tfidf_vectorize,\n",
    "    \"LDA\": LDA_vectorize,\n",
    "    \"word2vec\": word2vec_vectorize\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    \"SVM\": svc,\n",
    "    \"Naive Bayes\": gnb,\n",
    "    \"Decision Trees\": dec_tree,\n",
    "    \"Random Forest\": rfc,\n",
    "    \"Ada Boost classifier\": abc,\n",
    "    \"MLP\": mlpc\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_light = [\"comp.graphics\", \"rec.autos\", \"sci.med\",  \"talk.politics.mideast\"]\n",
    "categories_hard = [\"talk.politics.guns\", \"talk.politics.mideast\",  \"talk.politics.misc\"]\n",
    "categories = {\"light\": fetch_20newsgroups(subset=\"all\",\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "                                categories=categories_light), \n",
    "              \"hard\": fetch_20newsgroups(subset=\"all\",\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "                                categories=categories_hard)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "light\n",
      "3893\n",
      "SVM\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.8847\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n",
      "F1 Score: 0.7824\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n",
      "F1 Score: 0.6786\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.8822\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n",
      "F1 Score: 0.8135\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n",
      "F1 Score: 0.7444\n",
      "Naive Bayes\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.8488\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n",
      "F1 Score: 0.7954\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n",
      "F1 Score: 0.6180\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.8542\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n",
      "F1 Score: 0.8129\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n",
      "F1 Score: 0.6149\n",
      "Decision Trees\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.7381\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n",
      "F1 Score: 0.7702\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n",
      "F1 Score: 0.6634\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.7412\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n",
      "F1 Score: 0.7015\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n",
      "F1 Score: 0.7037\n",
      "Random Forest\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.7534\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n",
      "F1 Score: 0.7639\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n",
      "F1 Score: 0.7161\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.7360\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n",
      "F1 Score: 0.7435\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n",
      "F1 Score: 0.7435\n",
      "Ada Boost classifier\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.7634\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n",
      "F1 Score: 0.8159\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n",
      "F1 Score: 0.7111\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.7478\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n",
      "F1 Score: 0.6458\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n",
      "F1 Score: 0.7453\n",
      "MLP\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.8836\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Andrey\\прога\\python\\document_classification\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8269\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Andrey\\прога\\python\\document_classification\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7543\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(3114, 5000) (779, 5000)\n",
      "F1 Score: 0.8771\n",
      "LDA\n",
      "(3114, 10) (779, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Andrey\\прога\\python\\document_classification\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8130\n",
      "word2vec\n",
      "(3114, 100) (779, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Andrey\\прога\\python\\document_classification\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7920\n",
      "hard\n",
      "2625\n",
      "SVM\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.8271\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n",
      "F1 Score: 0.6413\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.4385\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.8262\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n",
      "F1 Score: 0.6447\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.4694\n",
      "Naive Bayes\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.7408\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n",
      "F1 Score: 0.6178\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.5048\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.7354\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n",
      "F1 Score: 0.6546\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.5761\n",
      "Decision Trees\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.7058\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n",
      "F1 Score: 0.5770\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.5674\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.6732\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n",
      "F1 Score: 0.5520\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.5649\n",
      "Random Forest\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.6240\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n",
      "F1 Score: 0.6215\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.5707\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.6122\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n",
      "F1 Score: 0.6512\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.6486\n",
      "Ada Boost classifier\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.6869\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n",
      "F1 Score: 0.6133\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.6126\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.6803\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n",
      "F1 Score: 0.5610\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.6476\n",
      "MLP\n",
      "start calculate ALL\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.8004\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Andrey\\прога\\python\\document_classification\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6105\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.6355\n",
      "start calculate NOUNS and ADJ\n",
      "tfidf\n",
      "(2100, 5000) (525, 5000)\n",
      "F1 Score: 0.8105\n",
      "LDA\n",
      "(2100, 10) (525, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Andrey\\прога\\python\\document_classification\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6624\n",
      "word2vec\n",
      "(2100, 100) (525, 100)\n",
      "F1 Score: 0.6723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Andrey\\прога\\python\\document_classification\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "folder = \"./20news_classifier/\"\n",
    "\n",
    "for data in categories:\n",
    "    print(data)\n",
    "    df = main(categories[data], tokinizers, classifiers)\n",
    "    df.to_excel(folder + data + \".xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
